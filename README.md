# FLARE
<h1>Forced Labor Automated Risk Estimator, a supply chain classifier</h1>

This classifer depends on data that I can share with others with the implementation of a Data Sharing Agreement (it is generally permissive, but my organization requires it so we don't get sued). For a copy of the underlying data, email me at shannon at gfems dot org.

This model reflects the following key decisions, which could have reasonably been made a different way, and I invite you to use your own judgement to adapt these principles to your own needs.

1. This model aims to quantify risk at the import/export step. The underlying data is likely to be biased towards companies that have international trade relationships, as NGOs and journalists focus their efforts there for maximum impact. I would expect the tool to be less accurate in evaluating firms that serve only a domestic market.
2. The challenge of using open-source data is that entities will appear in slight variations in each list. For the purposes of this model, an 'entity' includes all businesses registered by the same (or overlapping subsets of) directors, operating in the same industry, located at the same address or contactable through the same email address. This would include a subsidiary of a garment company that specializes in cotton elastic tape, even if the name is quite different from the parent company. If you implement a similar tool, you might make a different choice, but I felt that labor conditions at the two businesses were likely to be substantially similar.
3. Training data uses two possible criteria: either an entity has been found guilty of a crime analogous to forced labor by a labor tribunal, or there are at least two independent reports by journalists or NGOs that find forced labor conditions at a firm within the five years before data collection began. I never second-guess the first because I assume they have received due process, and the second is the standard used by the financial industry database Refinitiv.
4. We know from prevalence estimates in high-risk industries that the rate of identifiable forced labor is around 5-10% at the worker level, but it is generally detected at a rate of about .01% at the firm level. Therefore, it becomes difficult to truly assess the accuracy of a tool like this. Its accuracy is now measured by internal comparison, but in general, it would be better to undertake an expensive but thorough experiment to train the tool adaptively.

This project could be divided into three major phases: 1) landscaping and data collection 2) technical execution and 3) stakeholder engagement and socialization. 

<h2>Landscaping and data collection</h2>

From the outset, we knew that data collection would take at least several months to get from identification to data transfers, so we conducted a thorough landscaping exercise in parallel. 

The landscaping exercise turned out to be critically important to our success. We learned from this exactly what the state of the art was in the field and that some solutions existed, but they were often based on closely-guarded audit data, or they used completely uninformative training data. As a result, potential clients of the tool were sophisticated consumers who are rightly skeptical of promises made by each new tool and approach. However, there was no established minimum baseline tool, available to all. This experience still informs our pitches today.

On data collection, one major challenge we encountered was identifying a geography and sector where there was a discernible difference between firms who used forced labor as a business model and those that didn’t. We had some false starts with palm oil (we could not identify any sources of palm oil that could be reasonably surely declared free of forced labor, or even unknown) and fishing (fish go through a chaotic market, often right at ports, and supply chain relationships are not stable from day to day). While many people have asked if it is possible to identify, for instance, cocoa from West Africa that is made with child labor using machine learning, it is important to remember that there must be at least two categories of firms in a dataset to train a machine learning model (among low-risk, unknown, and high-risk). If all known firms in a geography or industry are high-risk, the model has nothing to learn.

<h2>Technical Execution</h2>

The technical execution was particularly enabled by identifying a contractor with experience gathering data from developing nation contexts, where data is often poorly structured, encoded using archaic character sets, and distributed across web domains that do not bear a clear relationship to one another. To gather and structure this data requires persistence and creativity, and it is propelled more quickly by teams that have experience overcoming the types of challenges that typically arise. This process was so valuable that as a final activity, we are creating a demonstration of how to work through this problem beginning from the point where a typical company would give up and declare the task impossible. The result of this effort is one of the largest structured datasets of forced labor cases that is publicly shareable.

Once a sufficient dataset was assembled, the model has always fundamentally worked. This means that most of the technical refinements, like the methods for handling rare events, did not have a large impact on the accuracy of the model. It is a good reason to believe that the associations we found between operational features and forced labor are probably reproducible and discoverable through multiple approaches. Its findings are also well aligned with the experience of other data scientists who have approached this problem. For instance, a city-level geographic risk feature seems to emerge from different types of data from different geographies and sectors, seemingly the result of an accretion of risk factors like local corruption, prevalence of migrant labor, and a high proportion of religious or ethnic minorities. It may be driven at least partly by the fact that most people do not move cities for work, creating strong city-level norms of working conditions. This observation is also consistent with Freedom Fund’s ‘hotspot’ model.

Another major technical achievement was in the area of entity recognition. Most companies who monitor supply chain risk have a very vertical conception of their supply chains, and they do not gather data from open sources because it is extremely difficult to connect that data to their internal lists of suppliers. A company may have several subsidiaries incorporated to specialize in different business functions (frequently a sister business handles all imports and exports, while the manufacturer is registered under a different name to the same owners at the same address). We applied a relatively new machine learning-based technique that had good success at identifying clusters of businesses owned by the same beneficial owners and operating with the same business purpose. Overcoming this hurdle may help those who monitor open sources like news reports better connect outside information to their supplier lists.

There are several important technical caveats that will always apply to this model. 1) This model is designed to be a decision support tool, not an executioner. The responsible sourcing manager has limited time and budget to achieve the largest impact she can, and this tool is meant to prioritize her efforts in the absence of any other good information. A finding of forced labor should rely on more careful and detailed information, such as social audits or worker voice. 2) The underlying data are informed by current priorities, and the tool cannot see beyond the data it was trained with. Most investigations of firms in India are driven by the connections they have with large international brands, so firms that serve the domestic market are less surveilled by journalists and NGOs. 3) This model was trained on data from before the Covid-19 pandemic. By February of 2020, our data collection was already well underway, and our first working prototype was achieved in the summer of 2020. Since then, there have been major supply chain disruptions and worker shortages, and we do not know for sure how this has impacted firm-level risk of forced labor. 4) This model may not be specific to forced labor. In my experience, there are a host of correlated behaviors that emerge concurrently in businesses that are fundamentally unsound, such as tax avoidance, environmental violations, and other unethical business practices. This model was trained on a forced labor dataset, but it was not counter-trained on businesses known to be doing other crimes but not forced labor. So, it is possible it is detecting a generalized business risk that is not specific to forced labor. It will still work for forced labor, but only as long as forced labor remains correlated with other business risks.

<h2>Stakeholder engagement and socialization</h2>

Finally, once we had a working prototype, we were able to socialize the idea. Our proof of concept showed skeptics that it is possible to identify signals of forced labor from open-source data, meaning anyone–inside or outside a company’s supply chain, including regulators, can discover a good lead and investigate. That said, it would be a year from those early conversations before companies would invest budgets into replicating our results.

The other major driver of progress in the year 2021 was increasing regulatory pressure on real goods made with forced labor. The US stepped up enforcement of the Tariff Act prohibition on goods made with forced labor, and several other countries considered similar legislation. Further, the US passed and signed the Uyghur Forced Labor Prevention Act, which, if implemented similarly, will have far-reaching consequences for business. GFEMS was able to publicize the effects of withhold release orders (the enforcement mechanism of the Tariff Act) at a webinar at which an executive from a company who had received a WRO said it had cost their business $40 million, six months of imports, and that, aside from that, it had been the worst thing that had ever happened to him personally.

<h2>Final thoughts</h2>

This tool has some obvious potential and attracts interest from funders, but one thing I would emphasize is that <i>perpetual maintenance at philanthropic expense is not a desirable outcome</i>. The internet is rife with dead or derelict tools developed by philanthropies that did not find a suitable end of life. This tool, if it works, provides a valuable business service, and businesses should shoulder the expense of maintaining and updating it. To capture that commercial market, its best path forward is to be integrated with platforms that are already in use or address a larger business need than the identification of forced labor, such as holistic ESG or due diligence concerns. 
